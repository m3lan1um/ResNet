{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리"
      ],
      "metadata": {
        "id": "QdTo6tt27Ps_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zN2oNfSp880n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 디바이스 선택"
      ],
      "metadata": {
        "id": "Uap1Ajv67VkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "fxigi8VuupYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d1bb70-ed5c-4f2f-8892-0c9dd00a67d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 구글 드라이브 마운트"
      ],
      "metadata": {
        "id": "5qb4xEhN7S67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA_KLcX09OeS",
        "outputId": "e579d55d-1108-44c3-e50f-7b9cd90c527e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 다운받기"
      ],
      "metadata": {
        "id": "F6qQtcP17f2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = v2.Compose([\n",
        "    v2.Resize(224),\n",
        "    v2.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "Y7jVDDu-QbFa",
        "outputId": "a6343fd1-7dc0-4711-9cde-a0901f521924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/MyDrive/Colab Notebooks/DL_impl/ResNet/datasets'\n",
        "\n",
        "train_ds = datasets.CIFAR10(root, train=True, transform=transforms, download=True)\n",
        "test_ds = datasets.CIFAR10(root, train=False, transform=transforms, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB7EIZm3-QN9",
        "outputId": "44d09813-ce13-4075-e1b0-977e773a568a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 크기 확인\n",
        "print(f'train dataset size: {len(train_ds)}')\n",
        "print(f'test dataset size: {len(test_ds)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u91TQkJEAPVt",
        "outputId": "8c7cf6a0-9ab6-48b7-c35e-0671e1dee0bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset size: 50000\n",
            "test dataset size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 정의하기\n",
        "- 원본 코드에서 전반적인 구조를 참고했으나, 당장 필요해 보이는 부분을 제외하고는 제거하였음.\n",
        "- 또한 downsampling 하는 과정에서 원본은 downsample 인자로 `Conv2d` 또는 `None`을 전달하지만, 나는 Bool 타입으로 전달하고 내부적으로 생성하는 방식을 이용했음.  \n",
        "  - 내가 했을 때 생각나는 대로 했음.\n",
        "  - 추후에 변경할 수도 있음."
      ],
      "metadata": {
        "id": "yYaHOfvz7kGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, downsample):\n",
        "    super().__init__()\n",
        "\n",
        "    self.stride = (2, 1) if downsample else (1, 1)\n",
        "    self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=self.stride[0], padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=self.stride[1], padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    self.downsample = None if not downsample else nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=2),\n",
        "        nn.BatchNorm2d(out_channel)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x if not self.downsample else self.downsample(x)\n",
        "    res = self.conv1(x)\n",
        "    res = self.bn1(res)\n",
        "    res = self.relu1(res)\n",
        "    res = self.conv2(res)\n",
        "    res = self.bn2(res)\n",
        "    out = res + identity\n",
        "    out = self.relu2(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "pHqln7yS0eM3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34(nn.Module):\n",
        "  def __init__(self, block, blocks_per_layer, n_targets):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_layer = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    )\n",
        "\n",
        "    self.layer1 = self._make_layer(block, 64, 64, blocks_per_layer[0])   # 이것만 downsampling 없음. 특징: in_channel == out_channel\n",
        "    self.layer2 = self._make_layer(block, 64, 128, blocks_per_layer[1])\n",
        "    self.layer3 = self._make_layer(block, 128, 256, blocks_per_layer[2])\n",
        "    self.layer4 = self._make_layer(block, 256, 512, blocks_per_layer[3])\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512, n_targets)\n",
        "    )\n",
        "\n",
        "  def _make_layer(self, block, in_channel, out_channel, n_blocks):\n",
        "    layers = []\n",
        "\n",
        "    # 각 layer의 첫 번째 block\n",
        "    # 첫 번째 layer만 시작하는 블록의 filter가 stride=1\n",
        "    if in_channel == out_channel:\n",
        "      layers.append(block(in_channel, out_channel, downsample=False))\n",
        "    # 나머지 layer는 시작하는 블록의 filter가 strdie=2\n",
        "    else:\n",
        "      layers.append(block(in_channel, out_channel, downsample=True))\n",
        "\n",
        "    for _ in range(1, n_blocks):\n",
        "      layers.append(block(out_channel, out_channel, downsample=False))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "at5OXFMcI1EP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blocks_per_layer = [3, 4, 6, 3]\n",
        "n_targets = 10\n",
        "model = ResNet34(ResBlock, blocks_per_layer, n_targets).to(device)"
      ],
      "metadata": {
        "id": "zcLMMPUD7DNs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFTToqCq7IWP",
        "outputId": "0c7f0ece-91bb-4653-cbcd-f481b0114389"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchinfo\n",
        "\n",
        "batch_size = 256\n",
        "torchinfo.summary(model, input_size=(batch_size, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUAoz0-I7KCV",
        "outputId": "c0348faf-00e0-447a-e5b3-8af248db7760"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet34                                 [256, 10]                 --\n",
              "├─Sequential: 1-1                        [256, 64, 56, 56]         --\n",
              "│    └─Conv2d: 2-1                       [256, 64, 112, 112]       9,472\n",
              "│    └─BatchNorm2d: 2-2                  [256, 64, 112, 112]       128\n",
              "│    └─ReLU: 2-3                         [256, 64, 112, 112]       --\n",
              "│    └─MaxPool2d: 2-4                    [256, 64, 56, 56]         --\n",
              "├─Sequential: 1-2                        [256, 64, 56, 56]         --\n",
              "│    └─ResBlock: 2-5                     [256, 64, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-1                  [256, 64, 56, 56]         36,928\n",
              "│    │    └─BatchNorm2d: 3-2             [256, 64, 56, 56]         128\n",
              "│    │    └─ReLU: 3-3                    [256, 64, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-4                  [256, 64, 56, 56]         36,928\n",
              "│    │    └─BatchNorm2d: 3-5             [256, 64, 56, 56]         128\n",
              "│    │    └─ReLU: 3-6                    [256, 64, 56, 56]         --\n",
              "│    └─ResBlock: 2-6                     [256, 64, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-7                  [256, 64, 56, 56]         36,928\n",
              "│    │    └─BatchNorm2d: 3-8             [256, 64, 56, 56]         128\n",
              "│    │    └─ReLU: 3-9                    [256, 64, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-10                 [256, 64, 56, 56]         36,928\n",
              "│    │    └─BatchNorm2d: 3-11            [256, 64, 56, 56]         128\n",
              "│    │    └─ReLU: 3-12                   [256, 64, 56, 56]         --\n",
              "│    └─ResBlock: 2-7                     [256, 64, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-13                 [256, 64, 56, 56]         36,928\n",
              "│    │    └─BatchNorm2d: 3-14            [256, 64, 56, 56]         128\n",
              "│    │    └─ReLU: 3-15                   [256, 64, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-16                 [256, 64, 56, 56]         36,928\n",
              "│    │    └─BatchNorm2d: 3-17            [256, 64, 56, 56]         128\n",
              "│    │    └─ReLU: 3-18                   [256, 64, 56, 56]         --\n",
              "├─Sequential: 1-3                        [256, 128, 28, 28]        --\n",
              "│    └─ResBlock: 2-8                     [256, 128, 28, 28]        --\n",
              "│    │    └─Sequential: 3-19             [256, 128, 28, 28]        8,576\n",
              "│    │    └─Conv2d: 3-20                 [256, 128, 28, 28]        73,856\n",
              "│    │    └─BatchNorm2d: 3-21            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-22                   [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-23                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-24            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-25                   [256, 128, 28, 28]        --\n",
              "│    └─ResBlock: 2-9                     [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-26                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-27            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-28                   [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-29                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-30            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-31                   [256, 128, 28, 28]        --\n",
              "│    └─ResBlock: 2-10                    [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-32                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-33            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-34                   [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-35                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-36            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-37                   [256, 128, 28, 28]        --\n",
              "│    └─ResBlock: 2-11                    [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-38                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-39            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-40                   [256, 128, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-41                 [256, 128, 28, 28]        147,584\n",
              "│    │    └─BatchNorm2d: 3-42            [256, 128, 28, 28]        256\n",
              "│    │    └─ReLU: 3-43                   [256, 128, 28, 28]        --\n",
              "├─Sequential: 1-4                        [256, 256, 14, 14]        --\n",
              "│    └─ResBlock: 2-12                    [256, 256, 14, 14]        --\n",
              "│    │    └─Sequential: 3-44             [256, 256, 14, 14]        33,536\n",
              "│    │    └─Conv2d: 3-45                 [256, 256, 14, 14]        295,168\n",
              "│    │    └─BatchNorm2d: 3-46            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-47                   [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-48                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-49            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-50                   [256, 256, 14, 14]        --\n",
              "│    └─ResBlock: 2-13                    [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-51                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-52            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-53                   [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-54                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-55            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-56                   [256, 256, 14, 14]        --\n",
              "│    └─ResBlock: 2-14                    [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-57                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-58            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-59                   [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-60                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-61            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-62                   [256, 256, 14, 14]        --\n",
              "│    └─ResBlock: 2-15                    [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-63                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-64            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-65                   [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-66                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-67            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-68                   [256, 256, 14, 14]        --\n",
              "│    └─ResBlock: 2-16                    [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-69                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-70            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-71                   [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-72                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-73            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-74                   [256, 256, 14, 14]        --\n",
              "│    └─ResBlock: 2-17                    [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-75                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-76            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-77                   [256, 256, 14, 14]        --\n",
              "│    │    └─Conv2d: 3-78                 [256, 256, 14, 14]        590,080\n",
              "│    │    └─BatchNorm2d: 3-79            [256, 256, 14, 14]        512\n",
              "│    │    └─ReLU: 3-80                   [256, 256, 14, 14]        --\n",
              "├─Sequential: 1-5                        [256, 512, 7, 7]          --\n",
              "│    └─ResBlock: 2-18                    [256, 512, 7, 7]          --\n",
              "│    │    └─Sequential: 3-81             [256, 512, 7, 7]          132,608\n",
              "│    │    └─Conv2d: 3-82                 [256, 512, 7, 7]          1,180,160\n",
              "│    │    └─BatchNorm2d: 3-83            [256, 512, 7, 7]          1,024\n",
              "│    │    └─ReLU: 3-84                   [256, 512, 7, 7]          --\n",
              "│    │    └─Conv2d: 3-85                 [256, 512, 7, 7]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-86            [256, 512, 7, 7]          1,024\n",
              "│    │    └─ReLU: 3-87                   [256, 512, 7, 7]          --\n",
              "│    └─ResBlock: 2-19                    [256, 512, 7, 7]          --\n",
              "│    │    └─Conv2d: 3-88                 [256, 512, 7, 7]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-89            [256, 512, 7, 7]          1,024\n",
              "│    │    └─ReLU: 3-90                   [256, 512, 7, 7]          --\n",
              "│    │    └─Conv2d: 3-91                 [256, 512, 7, 7]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-92            [256, 512, 7, 7]          1,024\n",
              "│    │    └─ReLU: 3-93                   [256, 512, 7, 7]          --\n",
              "│    └─ResBlock: 2-20                    [256, 512, 7, 7]          --\n",
              "│    │    └─Conv2d: 3-94                 [256, 512, 7, 7]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-95            [256, 512, 7, 7]          1,024\n",
              "│    │    └─ReLU: 3-96                   [256, 512, 7, 7]          --\n",
              "│    │    └─Conv2d: 3-97                 [256, 512, 7, 7]          2,359,808\n",
              "│    │    └─BatchNorm2d: 3-98            [256, 512, 7, 7]          1,024\n",
              "│    │    └─ReLU: 3-99                   [256, 512, 7, 7]          --\n",
              "├─AdaptiveAvgPool2d: 1-6                 [256, 512, 1, 1]          --\n",
              "├─Sequential: 1-7                        [256, 10]                 --\n",
              "│    └─Flatten: 2-21                     [256, 512]                --\n",
              "│    └─Linear: 2-22                      [256, 10]                 5,130\n",
              "==========================================================================================\n",
              "Total params: 21,298,314\n",
              "Trainable params: 21,298,314\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 938.75\n",
              "==========================================================================================\n",
              "Input size (MB): 154.14\n",
              "Forward/backward pass size (MB): 15311.33\n",
              "Params size (MB): 85.19\n",
              "Estimated Total Size (MB): 15550.66\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 & 검증"
      ],
      "metadata": {
        "id": "5MzEvpjP91Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=.1, momentum=.9, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "ZCo05hd2-AfX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dl, transforms, model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  for idx, (X, y) in enumerate(dl):\n",
        "    X = transforms(X)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    correct = (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if idx % 10 == 0 :\n",
        "      print(f'Train loss: {loss.item()} -- Train correct: {correct / len(X)}')\n",
        "\n",
        "def validate(dl, model, loss_fn):\n",
        "  loss, correct = 0, 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for X, y in dl:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      pred = model(X)\n",
        "      loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  print(f'Val loss: {loss / len(dl)} -- Val correct: {correct / len(dl.dataset)}')"
      ],
      "metadata": {
        "id": "6w312Cdl92q8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
        "test_dl = DataLoader(test_ds, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Igdz1oKYGJ3T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 5\n",
        "\n",
        "for e in range(eps):\n",
        "  print(f'===== epoch: {(e + 1)}/{eps} =====')\n",
        "  train(train_dl, transforms, model, loss_fn, optimizer)\n",
        "  validate(test_dl, model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhe6ZAbTG8Ws",
        "outputId": "330197a0-c9d4-41e2-cc44-99006a3bc279"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== epoch: 1/5 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.6265506744384766 -- Train correct: 0.08203125\n",
            "Train loss: 3.1622865200042725 -- Train correct: 0.09375\n",
            "Train loss: 3.0855393409729004 -- Train correct: 0.11328125\n",
            "Train loss: 2.4562137126922607 -- Train correct: 0.12890625\n",
            "Train loss: 2.3425276279449463 -- Train correct: 0.1484375\n",
            "Train loss: 2.3114895820617676 -- Train correct: 0.12890625\n",
            "Train loss: 2.243732213973999 -- Train correct: 0.171875\n",
            "Train loss: 2.267669439315796 -- Train correct: 0.19140625\n",
            "Train loss: 2.1771304607391357 -- Train correct: 0.18359375\n",
            "Train loss: 2.037295341491699 -- Train correct: 0.22265625\n",
            "Train loss: 2.151080846786499 -- Train correct: 0.19921875\n",
            "Train loss: 2.0738768577575684 -- Train correct: 0.23828125\n",
            "Train loss: 2.046452045440674 -- Train correct: 0.2265625\n",
            "Train loss: 2.0712244510650635 -- Train correct: 0.265625\n",
            "Train loss: 1.9917190074920654 -- Train correct: 0.2421875\n",
            "Train loss: 1.9757195711135864 -- Train correct: 0.29296875\n",
            "Train loss: 1.9731031656265259 -- Train correct: 0.2578125\n",
            "Train loss: 1.8402272462844849 -- Train correct: 0.26953125\n",
            "Train loss: 1.8699480295181274 -- Train correct: 0.3125\n",
            "Train loss: 1.9062343835830688 -- Train correct: 0.29296875\n",
            "Val loss: 1.9027976840734482 -- Val correct: 0.3182\n",
            "===== epoch: 2/5 =====\n",
            "Train loss: 1.7664107084274292 -- Train correct: 0.33984375\n",
            "Train loss: 1.7808387279510498 -- Train correct: 0.3125\n",
            "Train loss: 1.859426498413086 -- Train correct: 0.3359375\n",
            "Train loss: 1.7609182596206665 -- Train correct: 0.33984375\n",
            "Train loss: 1.7969602346420288 -- Train correct: 0.34375\n",
            "Train loss: 1.753224492073059 -- Train correct: 0.3671875\n",
            "Train loss: 1.873626708984375 -- Train correct: 0.296875\n",
            "Train loss: 1.7417054176330566 -- Train correct: 0.33984375\n",
            "Train loss: 1.6665302515029907 -- Train correct: 0.375\n",
            "Train loss: 1.6245017051696777 -- Train correct: 0.33984375\n",
            "Train loss: 1.6754783391952515 -- Train correct: 0.38671875\n",
            "Train loss: 1.6259750127792358 -- Train correct: 0.39453125\n",
            "Train loss: 1.6410102844238281 -- Train correct: 0.375\n",
            "Train loss: 1.6730566024780273 -- Train correct: 0.37890625\n",
            "Train loss: 1.6376818418502808 -- Train correct: 0.41015625\n",
            "Train loss: 1.5827510356903076 -- Train correct: 0.40234375\n",
            "Train loss: 1.6357496976852417 -- Train correct: 0.375\n",
            "Train loss: 1.5432090759277344 -- Train correct: 0.40625\n",
            "Train loss: 1.5690357685089111 -- Train correct: 0.453125\n",
            "Train loss: 1.5998647212982178 -- Train correct: 0.421875\n",
            "Val loss: 1.5757937520742415 -- Val correct: 0.4196\n",
            "===== epoch: 3/5 =====\n",
            "Train loss: 1.4974819421768188 -- Train correct: 0.4453125\n",
            "Train loss: 1.5759780406951904 -- Train correct: 0.4375\n",
            "Train loss: 1.5858570337295532 -- Train correct: 0.4140625\n",
            "Train loss: 1.6063743829727173 -- Train correct: 0.3671875\n",
            "Train loss: 1.5963820219039917 -- Train correct: 0.4296875\n",
            "Train loss: 1.5122629404067993 -- Train correct: 0.44921875\n",
            "Train loss: 1.677235722541809 -- Train correct: 0.390625\n",
            "Train loss: 1.5406047105789185 -- Train correct: 0.43359375\n",
            "Train loss: 1.4884146451950073 -- Train correct: 0.40234375\n",
            "Train loss: 1.459415316581726 -- Train correct: 0.45703125\n",
            "Train loss: 1.41923987865448 -- Train correct: 0.4765625\n",
            "Train loss: 1.399975299835205 -- Train correct: 0.50390625\n",
            "Train loss: 1.5002659559249878 -- Train correct: 0.421875\n",
            "Train loss: 1.4517240524291992 -- Train correct: 0.46484375\n",
            "Train loss: 1.408403754234314 -- Train correct: 0.49609375\n",
            "Train loss: 1.4009557962417603 -- Train correct: 0.4453125\n",
            "Train loss: 1.430801272392273 -- Train correct: 0.4609375\n",
            "Train loss: 1.3957346677780151 -- Train correct: 0.44921875\n",
            "Train loss: 1.3510985374450684 -- Train correct: 0.53125\n",
            "Train loss: 1.391279697418213 -- Train correct: 0.484375\n",
            "Val loss: 1.3784760624170302 -- Val correct: 0.4935\n",
            "===== epoch: 4/5 =====\n",
            "Train loss: 1.2492619752883911 -- Train correct: 0.5390625\n",
            "Train loss: 1.3791673183441162 -- Train correct: 0.51171875\n",
            "Train loss: 1.464231252670288 -- Train correct: 0.47265625\n",
            "Train loss: 1.403853416442871 -- Train correct: 0.48046875\n",
            "Train loss: 1.5013175010681152 -- Train correct: 0.42578125\n",
            "Train loss: 1.3569952249526978 -- Train correct: 0.46875\n",
            "Train loss: 1.4565099477767944 -- Train correct: 0.48828125\n",
            "Train loss: 1.278177261352539 -- Train correct: 0.5390625\n",
            "Train loss: 1.3324930667877197 -- Train correct: 0.52734375\n",
            "Train loss: 1.2661174535751343 -- Train correct: 0.53515625\n",
            "Train loss: 1.2965774536132812 -- Train correct: 0.54296875\n",
            "Train loss: 1.1975163221359253 -- Train correct: 0.59375\n",
            "Train loss: 1.3184213638305664 -- Train correct: 0.53125\n",
            "Train loss: 1.259329915046692 -- Train correct: 0.5546875\n",
            "Train loss: 1.227670431137085 -- Train correct: 0.546875\n",
            "Train loss: 1.2316465377807617 -- Train correct: 0.51171875\n",
            "Train loss: 1.2541600465774536 -- Train correct: 0.515625\n",
            "Train loss: 1.2028498649597168 -- Train correct: 0.56640625\n",
            "Train loss: 1.2043431997299194 -- Train correct: 0.55078125\n",
            "Train loss: 1.1905220746994019 -- Train correct: 0.55859375\n",
            "Val loss: 1.2901101857423782 -- Val correct: 0.5341\n",
            "===== epoch: 5/5 =====\n",
            "Train loss: 1.0313668251037598 -- Train correct: 0.62890625\n",
            "Train loss: 1.1074532270431519 -- Train correct: 0.59765625\n",
            "Train loss: 1.28512442111969 -- Train correct: 0.5625\n",
            "Train loss: 1.197007656097412 -- Train correct: 0.5546875\n",
            "Train loss: 1.2746469974517822 -- Train correct: 0.5625\n",
            "Train loss: 1.2005038261413574 -- Train correct: 0.5390625\n",
            "Train loss: 1.139803171157837 -- Train correct: 0.59765625\n",
            "Train loss: 1.1324541568756104 -- Train correct: 0.578125\n",
            "Train loss: 1.207387924194336 -- Train correct: 0.55859375\n",
            "Train loss: 1.0830491781234741 -- Train correct: 0.62890625\n",
            "Train loss: 1.1207127571105957 -- Train correct: 0.58203125\n",
            "Train loss: 1.0377918481826782 -- Train correct: 0.6328125\n",
            "Train loss: 1.1059390306472778 -- Train correct: 0.61328125\n",
            "Train loss: 1.09022855758667 -- Train correct: 0.57421875\n",
            "Train loss: 1.0367999076843262 -- Train correct: 0.67578125\n",
            "Train loss: 1.0974234342575073 -- Train correct: 0.58984375\n",
            "Train loss: 1.1103248596191406 -- Train correct: 0.6015625\n",
            "Train loss: 0.990281879901886 -- Train correct: 0.61328125\n",
            "Train loss: 1.0157713890075684 -- Train correct: 0.61328125\n",
            "Train loss: 0.9541195034980774 -- Train correct: 0.65234375\n",
            "Val loss: 1.1462765604257583 -- Val correct: 0.5927\n"
          ]
        }
      ]
    }
  ]
}